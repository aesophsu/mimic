import argparse
import json
import os
import sys
from typing import Any

import joblib
import numpy as np
import pandas as pd
from sklearn.calibration import CalibratedClassifierCV
from sklearn.metrics import accuracy_score, brier_score_loss, roc_auc_score
from sklearn.preprocessing import StandardScaler
from xgboost import XGBClassifier

sys.path.insert(0, os.path.join(os.path.dirname(__file__), ".."))
from utils.logger import log as _log, log_header
from utils.paths import get_cleaned_path, get_model_dir, ensure_dirs
from utils.study_config import OUTCOMES


TRAIN_PATH = get_cleaned_path("mimic_train_processed.csv")
TEST_PATH = get_cleaned_path("mimic_test_processed.csv")


def _load_target_bundle(target: str) -> dict[str, Any]:
    bundle_path = os.path.join(get_model_dir(target), "deploy_bundle.pkl")
    if not os.path.exists(bundle_path):
        raise FileNotFoundError(f"ç¼ºå°‘ deploy_bundle: {bundle_path}")
    return joblib.load(bundle_path)


def _get_ranked_features(bundle: dict[str, Any]) -> list[str]:
    features = list(bundle.get("features") or bundle.get("feature_names") or [])
    if not features:
        raise ValueError("deploy_bundle ç¼ºå°‘ features")

    model = bundle.get("best_model")
    if model is None:
        return features

    try:
        if hasattr(model, "calibrated_classifiers_") and model.calibrated_classifiers_:
            cal_clf = model.calibrated_classifiers_[0]
            base = getattr(cal_clf, "estimator", getattr(cal_clf, "base_estimator", None))
        else:
            base = model
        importances = getattr(base, "feature_importances_", None)
        if importances is not None and len(importances) == len(features):
            pairs = sorted(zip(features, importances), key=lambda x: float(x[1]), reverse=True)
            return [p[0] for p in pairs]
    except Exception as e:
        _log(f"ç‰¹å¾é‡è¦æ€§è¯»å–å¤±è´¥ï¼ŒæŒ‰åŸé¡ºåºå›é€€: {e}", "WARN")
    return features


def _get_xgb_params(bundle: dict[str, Any]) -> dict[str, Any]:
    model = bundle.get("best_model")
    if model is None:
        return {"random_state": 42, "eval_metric": "logloss", "n_jobs": 1}

    try:
        if hasattr(model, "calibrated_classifiers_") and model.calibrated_classifiers_:
            cal_clf = model.calibrated_classifiers_[0]
            base = getattr(cal_clf, "estimator", getattr(cal_clf, "base_estimator", None))
        else:
            base = model
        params = dict(base.get_params())
        params["n_jobs"] = 1  # æ²™ç®±/æœ¬åœ°ç¨³å®šè¿è¡Œ
        return params
    except Exception:
        return {"random_state": 42, "eval_metric": "logloss", "n_jobs": 1}


def _fit_eval_topk(
    X_train_raw: pd.DataFrame,
    y_train: np.ndarray,
    X_test_raw: pd.DataFrame,
    y_test: np.ndarray,
    xgb_params: dict[str, Any],
) -> dict[str, float]:
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train_raw)
    X_test = scaler.transform(X_test_raw)

    model = XGBClassifier(**xgb_params)
    clf = CalibratedClassifierCV(model, cv=3, method="isotonic", n_jobs=1)
    clf.fit(X_train, y_train)
    y_prob = clf.predict_proba(X_test)[:, 1]
    y_pred = (y_prob >= 0.5).astype(int)

    return {
        "auc": float(roc_auc_score(y_test, y_prob)),
        "accuracy": float(accuracy_score(y_test, y_pred)),
        "brier": float(brier_score_loss(y_test, y_prob)),
    }


def _run_target(
    target: str,
    df_train: pd.DataFrame,
    df_test: pd.DataFrame,
    min_k: int,
    max_k: int | None,
    auc_tolerance: float,
) -> tuple[pd.DataFrame, dict[str, Any]]:
    bundle = _load_target_bundle(target)
    ranked_features = _get_ranked_features(bundle)
    xgb_params = _get_xgb_params(bundle)

    usable = [f for f in ranked_features if f in df_train.columns and f in df_test.columns]
    if not usable:
        raise ValueError(f"{target}: æ— å¯ç”¨ç‰¹å¾")

    y_train = df_train[target].dropna().astype(int)
    y_test = df_test[target].dropna().astype(int)
    X_train_base = df_train.loc[y_train.index, usable]
    X_test_base = df_test.loc[y_test.index, usable]

    k_min = max(1, min_k)
    k_max = len(usable) if max_k is None else min(max_k, len(usable))
    if k_min > k_max:
        raise ValueError(f"{target}: min_k({k_min}) > max_k({k_max})")

    rows = []
    for k in range(k_min, k_max + 1):
        feats = usable[:k]
        metrics = _fit_eval_topk(X_train_base[feats], y_train.values, X_test_base[feats], y_test.values, xgb_params)
        rows.append(
            {
                "target": target,
                "k": k,
                "features": ";".join(feats),
                "auc": metrics["auc"],
                "accuracy": metrics["accuracy"],
                "brier": metrics["brier"],
                "n_test": int(len(y_test)),
            }
        )
        _log(
            f"[{target}] k={k:>2d} | AUC={metrics['auc']:.4f} | ACC={metrics['accuracy']:.4f} | Brier={metrics['brier']:.4f}",
            "INFO",
        )

    curve = pd.DataFrame(rows).sort_values("k").reset_index(drop=True)
    best_auc = float(curve["auc"].max())
    threshold = best_auc - auc_tolerance
    candidates = curve[curve["auc"] >= threshold].sort_values("k")
    picked = candidates.iloc[0]
    best_row = curve.sort_values(["auc", "k"], ascending=[False, True]).iloc[0]

    recommendation = {
        "target": target,
        "k_recommended": int(picked["k"]),
        "auc_recommended": float(picked["auc"]),
        "accuracy_recommended": float(picked["accuracy"]),
        "k_best_auc": int(best_row["k"]),
        "auc_best": float(best_row["auc"]),
        "auc_tolerance": float(auc_tolerance),
        "selection_rule": "smallest_k_within_auc_tolerance_on_internal_test",
    }
    return curve, recommendation


def main() -> None:
    parser = argparse.ArgumentParser(description="ä»…å¼€å‘é›†å†… XGBoost é€’å‡ç‰¹å¾ç­›é€‰ï¼ˆé™„åŠ åˆ†æï¼‰")
    parser.add_argument("--targets", nargs="+", default=OUTCOMES, help=f"ç»“å±€åˆ—è¡¨ï¼Œé»˜è®¤: {' '.join(OUTCOMES)}")
    parser.add_argument("--min-k", type=int, default=3, help="æœ€å°ç‰¹å¾æ•°")
    parser.add_argument("--max-k", type=int, default=None, help="æœ€å¤§ç‰¹å¾æ•°ï¼ˆé»˜è®¤ä½¿ç”¨å…¨éƒ¨å·²é€‰ç‰¹å¾ï¼‰")
    parser.add_argument("--auc-tolerance", type=float, default=0.01, help="AUC å®¹å¿å·®å€¼ï¼ˆæ¨èæœ€å°ké˜ˆå€¼ï¼‰")
    args = parser.parse_args()

    log_header("ğŸ§ª 06b_xgb_internal_feature_pruning: å¼€å‘é›†å†…é™„åŠ å˜é‡ç­›é€‰")
    _log("è¯´æ˜ï¼šæœ¬è„šæœ¬ä»…ä½¿ç”¨ MIMIC å¼€å‘é›†ï¼ˆè®­ç»ƒ+å†…éƒ¨æµ‹è¯•ï¼‰ï¼Œä¸è¯»å– eICU å¤–éƒ¨é›†ã€‚", "INFO")
    _log(f"è®­ç»ƒé›†: {os.path.abspath(TRAIN_PATH)}", "INFO")
    _log(f"æµ‹è¯•é›†: {os.path.abspath(TEST_PATH)}", "INFO")

    if not os.path.exists(TRAIN_PATH) or not os.path.exists(TEST_PATH):
        raise FileNotFoundError("ç¼ºå°‘ mimic_train_processed.csv æˆ– mimic_test_processed.csvï¼Œè¯·å…ˆè¿è¡Œ Step 02")

    df_train = pd.read_csv(TRAIN_PATH)
    df_test = pd.read_csv(TEST_PATH)

    targets = [t.lower() for t in args.targets]
    summary_rows = []

    for target in targets:
        if target not in df_train.columns or target not in df_test.columns:
            _log(f"è·³è¿‡ {target}: ç»“å±€åˆ—ä¸å­˜åœ¨", "WARN")
            continue

        target_dir = get_model_dir(target)
        ensure_dirs(target_dir)
        curve, rec = _run_target(target, df_train, df_test, args.min_k, args.max_k, args.auc_tolerance)

        curve_path = os.path.join(target_dir, "xgb_internal_pruning_curve.csv")
        rec_path = os.path.join(target_dir, "xgb_internal_pruning_recommendation.json")
        curve.to_csv(curve_path, index=False)
        with open(rec_path, "w", encoding="utf-8") as f:
            json.dump(rec, f, ensure_ascii=False, indent=2)

        _log(f"[{target}] æ›²çº¿ä¿å­˜: {os.path.abspath(curve_path)}", "OK")
        _log(f"[{target}] æ¨èä¿å­˜: {os.path.abspath(rec_path)}", "OK")
        summary_rows.append(rec)

    if summary_rows:
        summary = pd.DataFrame(summary_rows)
        summary_path = os.path.join(get_model_dir(), "xgb_internal_pruning_summary.csv")
        summary.to_csv(summary_path, index=False)
        _log(f"æ±‡æ€»ä¿å­˜: {os.path.abspath(summary_path)}", "OK")
    else:
        _log("æœªç”Ÿæˆä»»ä½•ç»“æœï¼ˆè¯·æ£€æŸ¥ targets å‚æ•°ï¼‰", "WARN")


if __name__ == "__main__":
    main()
