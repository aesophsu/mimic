import os
import sys
import json
import joblib
import re
import matplotlib.patches as mpatches
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, confusion_matrix, f1_score, roc_auc_score
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))
from utils.feature_formatter import FeatureFormatter
from utils.study_config import OUTCOMES, OUTCOME_TYPE
from utils.paths import get_project_root, get_model_dir, get_main_table_dir, get_supplementary_table_dir, get_supplementary_figure_dir, ensure_dirs
from utils.logger import log as _log, log_header
from utils.plot_config import (
    apply_medical_style, SAVE_DPI, PALETTE_MAIN, COLOR_POSITIVE, COLOR_NEGATIVE,
    COLOR_REF_LINE, OR_POINT_COLOR, FIG_WIDTH_DOUBLE, save_fig_medical
)

BASE_DIR = get_project_root()
MODEL_ROOT = get_model_dir()
FIG_CUTOFF_DIR = get_supplementary_figure_dir("S3_cutoff")
TABLE_ROOT = get_main_table_dir()
SUPP_TABLE_DIR = get_supplementary_table_dir()
ensure_dirs(FIG_CUTOFF_DIR, TABLE_ROOT, SUPP_TABLE_DIR)

def calculate_detailed_metrics(y_true, y_prob, threshold):
    y_pred = (y_prob >= threshold).astype(int)
    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
    
    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0
    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
    
    metrics = {
        "Threshold": round(threshold, 4),
        "TP": int(tp), "FP": int(fp), "TN": int(tn), "FN": int(fn),
        "Sensitivity": round(sensitivity, 4),
        "Specificity": round(specificity, 4),
        "PPV": round(tp / (tp + fp), 4) if (tp + fp) > 0 else 0,
        "NPV": round(tn / (tn + fn), 4) if (tn + fn) > 0 else 0,
        "F1_Score": round(f1_score(y_true, y_pred), 4),
        "Accuracy": round((tp + tn) / (tp + tn + fp + fn), 4)
        # "Sen_CI": "N/A"  # å¦‚æœä¸è·‘ Bootstrapï¼Œå»ºè®®å…ˆæ³¨é‡Šæ‰æˆ–è®¾ä¸º N/A
    }
    return metrics

def plot_diagnostic_viz(y_true, y_prob, threshold, name, target, save_dir):
    """è¯Šæ–­å›¾ï¼šåŒ»å­¦æœŸåˆŠæ ¼å¼ï¼ˆåŒæ ã€DPI 600ï¼‰"""
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)
    apply_medical_style()
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(FIG_WIDTH_DOUBLE * 1.5, 5), dpi=300, facecolor='white')
    c_normal = COLOR_NEGATIVE
    c_event = COLOR_POSITIVE
    c_main = OR_POINT_COLOR
    fpr, tpr, _ = roc_curve(y_true, y_prob)
    auc_val = roc_auc_score(y_true, y_prob)
    auc_label = f'{name} (AUC = {auc_val:.3f})'
    ci_lo, ci_hi = None, None
    ci_path = os.path.join(get_model_dir(target), "bootstrap_ci_stats.pkl")
    if os.path.exists(ci_path):
        try:
            boot_stats = joblib.load(ci_path)
            if name in boot_stats and 'main' in boot_stats[name]:
                ci_lo, ci_hi = boot_stats[name]['main']
                auc_label = f"{name} (AUC = {auc_val:.3f}, 95% CI: {ci_lo:.3f}-{ci_hi:.3f})"
        except Exception:
            pass

    ax1.plot(fpr, tpr, label=auc_label, color=c_main, lw=2.5)
    ax1.plot([0, 1], [0, 1], linestyle='--', color=COLOR_REF_LINE, alpha=0.5, lw=1)

    text_auc = f'AUC = {auc_val:.3f} (95% CI: {ci_lo:.3f}-{ci_hi:.3f})' if (ci_lo is not None and ci_hi is not None) else f'AUC = {auc_val:.3f}'
    ax1.text(0.6, 0.1, text_auc, fontsize=12, 
             fontweight='bold', color=c_main, bbox=dict(facecolor='white', alpha=0.8, edgecolor='none'))
    perf = calculate_detailed_metrics(y_true, y_prob, threshold)
    ax1.scatter(1-perf['Specificity'], perf['Sensitivity'], 
                color=c_event, s=120, edgecolors='white', zorder=5,
                label=f'Optimal Cutoff: {threshold:.3f}')
    ax1.annotate(f'Sensitivity: {perf["Sensitivity"]:.2f}\nSpecificity: {perf["Specificity"]:.2f}',
                 xy=(1-perf['Specificity'], perf['Sensitivity']), 
                 xytext=(1-perf['Specificity']+0.12, perf['Sensitivity']-0.15),
                 fontsize=10, fontweight='bold',
                 arrowprops=dict(arrowstyle="->", color='black', connectionstyle="arc3,rad=.2"))
    ax1.set_xlabel('1 - Specificity (False Positive Rate)', labelpad=10)
    ax1.set_ylabel('Sensitivity (True Positive Rate)', labelpad=10)
    ax1.set_title(f'Diagnostic Performance: {name}\n({target.upper()})', fontweight='bold', pad=15)
    ax1.legend(loc='lower right', frameon=False)
    ax1.set_aspect('equal')
    df_prob = pd.DataFrame({'prob': y_prob, 'target': y_true})
    sns.kdeplot(data=df_prob[df_prob['target'] == 0], x='prob', fill=True, 
                label='Normal/Survival', color=c_normal, ax=ax2, alpha=0.4, lw=2)
    sns.kdeplot(data=df_prob[df_prob['target'] == 1], x='prob', fill=True, 
                label='Outcome Event', color=c_event, ax=ax2, alpha=0.4, lw=2)
    ax2.axvline(threshold, color=c_main, linestyle='--', lw=2, alpha=0.8)
    ylim = ax2.get_ylim()[1]
    text_style = dict(fontsize=10, fontweight='bold', bbox=dict(facecolor='white', alpha=0.6, edgecolor='none', pad=1))
    
    ax2.text(threshold-0.03, ylim*0.92, 'LOW RISK', ha='right', color=c_normal, **text_style)
    ax2.text(threshold+0.03, ylim*0.85, 'HIGH RISK', ha='left', color=c_event, **text_style)
    ax2.set_xlabel('Predicted Risk Probability', labelpad=10)
    ax2.set_ylabel('Population Density', labelpad=10)
    ax2.set_title('Clinical Risk Stratification', fontweight='bold', pad=15)
    ax2.legend(frameon=False)
    sns.despine() # ç§»é™¤ä¸Šæ–¹å’Œå³ä¾§è¾¹æ¡†
    plt.tight_layout()
    save_filename = f"07_Diagnostic_{name}_{target}"
    save_base = os.path.join(save_dir, save_filename)
    save_fig_medical(save_base)
    plt.close()
    _log(f"è¯Šæ–­å›¾: {os.path.abspath(save_base)}", "OK")

def export_formatted_table3():
    summary_path = os.path.join(MODEL_ROOT, "global_diagnostic_summary.csv")
    if not os.path.exists(summary_path):
        _log("æœªæ‰¾åˆ°æ±‡æ€»è¡¨ï¼Œæ— æ³•å¯¼å‡º Table 3", "WARN"); return
    
    df = pd.read_csv(summary_path)
    formatted_rows = []
    ci_cache = {}
    
    for _, row in df.iterrows():
        target = str(row['Outcome']).lower()
        algo = row['Algorithm']
        group_raw = str(row['Group'])
        
        # 1. åŠ è½½ç¬¬ 06 æ­¥ CI èµ„äº§
        if target not in ci_cache:
            ci_path = os.path.join(get_model_dir(target), "bootstrap_ci_stats.pkl")
            ci_cache[target] = joblib.load(ci_path) if os.path.exists(ci_path) else None

        # 2. äººç¾¤åˆ†ç±»ä¸ CI ç´¢å¼•åŒ¹é…
        if 'Full' in group_raw:
            display_group, ci_key, group_order = 'Full Population', 'main', 0
        else:
            display_group, ci_key, group_order = 'Subgroup (No Renal)', 'sub', 1

        # 3. æ ¼å¼åŒ– AUC (95% CI)
        auc_val = row['AUC']
        auc_str = f"{auc_val:.3f}"
        target_ci = ci_cache.get(target)
        if target_ci and algo in target_ci:
            try:
                low, high = target_ci[algo][ci_key]
                auc_str = f"{auc_val:.3f} ({low:.3f}â€“{high:.3f})"
            except (KeyError, TypeError):
                pass 

        outcome_order = {'pof': 0, 'mortality': 1, 'composite': 2}.get(target, 9)
        formatted_rows.append({
            'Endpoint': OUTCOME_TYPE.get(target, target),
            'Outcome': row['Outcome'].upper(),
            'Group': display_group,
            'Model': algo,
            'AUC (95% CI)': auc_str,
            'Sens.': f"{row['Sensitivity']:.3f}",
            'Spec.': f"{row['Specificity']:.3f}",
            'F1': f"{row['F1_Score']:.3f}",
            'Optimal Cut-off': f"{row['Threshold']:.3f}",
            'auc_numeric': auc_val,
            'group_priority': group_order,
            'outcome_order': outcome_order
        })

    # 5. å¤šçº§é€»è¾‘æ’åºå¹¶å¯¼å‡º
    table3 = pd.DataFrame(formatted_rows)
    table3 = table3.sort_values(
        ['outcome_order', 'group_priority', 'auc_numeric'],
        ascending=[True, True, False]
    )

    final_columns = ['Endpoint', 'Outcome', 'Group', 'Model', 'AUC (95% CI)', 'Sens.', 'Spec.', 'F1', 'Optimal Cut-off']
    output_path = os.path.join(TABLE_ROOT, "Table3_performance.csv")
    table3[final_columns].to_csv(output_path, index=False, encoding='utf-8-sig')
    _log(f"Table 3: {os.path.abspath(output_path)}", "OK")
    
class ResultVisualizer:
    def __init__(self, base_dir=None):
        base_dir = base_dir or os.path.normpath(os.path.join(os.path.dirname(__file__), "../.."))
        self.model_root = os.path.join(base_dir, "artifacts/models")
        self.result_root = get_supplementary_figure_dir("S3_cutoff")
        self.report_path = os.path.join(self.model_root, "performance_report.csv")
        os.makedirs(self.result_root, exist_ok=True)
        
        apply_medical_style()
        self.sci_palette = PALETTE_MAIN

    def summarize_feature_importance(self, outcomes=None, top_n=15):
        outcomes = outcomes or OUTCOMES
        _log("æ­£åœ¨ç”Ÿæˆå‡ºç‰ˆçº§ç‰¹å¾é‡è¦æ€§å›¾...", "INFO")
        all_imps = []
        for target in outcomes:
            path = os.path.join(self.model_root, target, "feature_importance.csv")
            if os.path.exists(path):
                all_imps.append(pd.read_csv(path))
        
        if not all_imps: return

        full_df = pd.concat(all_imps, ignore_index=True)
        # è‹¥æ—  display_name åˆ—åˆ™è¡¥å……ï¼ˆå…¼å®¹æ—§ç‰ˆ feature_importance.csvï¼‰
        if 'display_name' not in full_df.columns:
            formatter = FeatureFormatter()
            full_df['display_name'] = full_df['feature'].map(lambda x: formatter.get_label(x))
        pivot_df = full_df.groupby(['feature', 'outcome'])['importance'].mean().unstack()
        pivot_df['Global_Avg'] = pivot_df.mean(axis=1)
        top_feats = pivot_df.sort_values('Global_Avg', ascending=False).head(top_n).index.tolist()
        feat_to_display = full_df.drop_duplicates('feature').set_index('feature')['display_name'].to_dict()
        top_display = [feat_to_display.get(f, f) for f in top_feats]

        fig, ax = plt.subplots(figsize=(FIG_WIDTH_DOUBLE, 6), dpi=300, facecolor='white')
        plot_data = full_df[full_df['feature'].isin(top_feats)]
        
        sns.barplot(data=plot_data, y='display_name', x='importance', hue='outcome', 
                    order=top_display, palette="mako", alpha=0.9, edgecolor="white", linewidth=0.5)
        
        # æ·»åŠ è½»é‡çº§å‚ç›´ç½‘æ ¼çº¿
        ax.xaxis.grid(True, linestyle='--', alpha=0.4, color='#CCCCCC')
        ax.set_axisbelow(True)

        plt.title("Primary Clinical Predictors of Outcomes", loc='left', pad=20)
        plt.xlabel("Mean Relative Feature Importance (Normalized)")
        plt.ylabel("")
        
        # ä¼˜åŒ–å›¾ä¾‹
        plt.legend(title="Clinical Outcome", frameon=False, loc='lower right', bbox_to_anchor=(1, 0.05))
        
        sns.despine()
        plt.tight_layout()
        save_base = os.path.join(self.result_root, "sci_feature_importance")
        save_fig_medical(save_base)
        plt.close()

    def plot_performance_forest(self):
        """åŒ»å­¦å‡ºç‰ˆçº§æ£®æ—å›¾ï¼ˆæ›´æ¸…æ™°çš„åˆ†ç»„ä¸å¯¹é½ï¼‰"""
        _log("æ­£åœ¨ç”Ÿæˆå‡ºç‰ˆçº§æ£®æ—å›¾...", "INFO")
        if not os.path.exists(self.report_path): return

        df = pd.read_csv(self.report_path)
        def parse_ci(s):
            vals = re.findall(r"([0-9.]+)", str(s))
            return [float(x) for x in vals[:3]] if len(vals) >= 3 else [np.nan]*3
        parsed = df['Main CI'].apply(parse_ci).tolist()
        df[['auc', 'low', 'high']] = pd.DataFrame(parsed, index=df.index)

        outcomes = df['Outcome'].unique()
        models = df['Algorithm'].unique()
        color_map = dict(zip(models, self.sci_palette))

        fig, ax = plt.subplots(figsize=(FIG_WIDTH_DOUBLE, 6), dpi=300, facecolor='white')
        
        y_pos = 0
        y_ticks, y_labels = [], []

        for outcome in reversed(outcomes):
            sub_df = df[df['Outcome'] == outcome]
            
            # ç»“å±€åˆ†ç»„æ¨ªå¸¦ (åŒ»å­¦æœŸåˆŠå¸¸ç”¨é£æ ¼)
            y_pos += 1
            ax.axhspan(y_pos-0.5, y_pos+len(sub_df)+0.5, color='#F8F9FA', alpha=0.8, zorder=0)
            
            # åˆ†ç»„æ ‡é¢˜
            ax.text(0.51, y_pos + (len(sub_df)/2) + 0.5, f"Outcome: {outcome.upper()}", 
                    va='center', ha='left', fontweight='black', fontsize=12, 
                    color='#2C3E50', style='italic')
            
            for _, row in sub_df.iterrows():
                y_pos += 1
                y_ticks.append(y_pos)
                y_labels.append(row['Algorithm'])
                
                # ç»˜åˆ¶ç½®ä¿¡åŒºé—´çº¿æ¡
                ax.plot([row['low'], row['high']], [y_pos, y_pos], color=color_map[row['Algorithm']], 
                        linewidth=2.5, solid_capstyle='round', zorder=3)
                
                # ç»˜åˆ¶ä¸­å¿ƒç‚¹ (Marker)
                ax.scatter(row['auc'], y_pos, color=color_map[row['Algorithm']], 
                           s=80, edgecolors='white', linewidth=1, zorder=4)
                
                # æ•°å€¼æ ‡æ³¨ (å¯¹é½å¯¹é½)
                label_text = f"{row['auc']:.3f} [{row['low']:.3f} - {row['high']:.3f}]"
                ax.text(1.01, y_pos, label_text, va='center', ha='left', fontsize=9.5, fontfamily='monospace')

            y_pos += 1.5 # ç»„é—´è·

        # å›¾è¡¨ä¿®é¥°
        ax.set_yticks(y_ticks)
        ax.set_yticklabels(y_labels, fontweight='normal')
        ax.axvline(0.5, color='#34495E', linestyle='-', linewidth=1.5, alpha=0.8, label='Chance level')
        ax.axvline(0.8, color='#BDC3C7', linestyle='--', linewidth=0.8, alpha=0.5)
        
        ax.set_xlabel('Area Under the ROC Curve (95% CI)', labelpad=15)
        ax.set_xlim(0.5, 1.0) # AUC ä¸å¯èƒ½è¶…è¿‡ 1.0
        
        # ç§»é™¤åæ ‡è½´å†—ä½™
        ax.spines['top'].set_visible(False)
        ax.spines['right'].set_visible(False)
        ax.spines['left'].set_visible(False)
        ax.xaxis.set_ticks_position('bottom')
        
        plt.title("Predictive Performance Across Clinical Outcomes", loc='left', pad=30, fontsize=16)
        
        # å›¾ä¾‹ç¾åŒ–
        patches = [mpatches.Patch(color=color_map[m], label=m) for m in models]
        ax.legend(handles=patches, loc='lower center', bbox_to_anchor=(0.5, -0.15), 
                  ncol=len(models), frameon=False, fontsize=9)

        plt.tight_layout()
        save_base = os.path.join(self.result_root, "sci_forest_plot")
        save_fig_medical(save_base)
        plt.close()
        _log(f"æ£®æ—å›¾: {os.path.abspath(save_base)}.png", "OK")

# 2. æ ¸å¿ƒæ‰§è¡Œé€»è¾‘
def run_cutoff_optimization_flow():
    log_header("ğŸš€ 07_optimal_cutoff_analysis: é˜ˆå€¼å¯»ä¼˜ä¸ä¸´åºŠæ•ˆèƒ½å®¡è®¡")
    _log(f"æ¨¡å‹ç›®å½•: {os.path.abspath(MODEL_ROOT)}", "INFO")
    global_summary = []

    for target in OUTCOMES:
        target_dir = get_model_dir(target)
        fig_save_dir = os.path.join(FIG_CUTOFF_DIR, target)
        
        if not os.path.exists(target_dir):
            _log(f"è·³è¿‡ {target}: è·¯å¾„ç¼ºå¤±", "WARN"); continue

        _log(f"æ­£åœ¨å¤„ç†ç»ˆç‚¹: [{target.upper()}]", "INFO")

        try:
            # 1. èµ„äº§åŠ è½½ä¸ç‰¹å¾å¯¹é½
            models_dict = joblib.load(os.path.join(target_dir, "all_models_dict.pkl"))
            eval_data = joblib.load(os.path.join(target_dir, "eval_data.pkl"))
            X_test_pre, y_test = eval_data['X_test_pre'], eval_data['y_test']
            feat_path = os.path.join(target_dir, "selected_features.json")
            if os.path.exists(feat_path):
                with open(feat_path, 'r') as f:
                    feat_data = json.load(f)
                
                # å…³é”®ä¿®å¤ï¼šä»å­—å…¸ä¸­æå– "features" åˆ—è¡¨
                if isinstance(feat_data, dict) and "features" in feat_data:
                    selected_features = feat_data["features"]
                else:
                    selected_features = list(feat_data) # å…œåº•é€»è¾‘
                
                # ä»…é€‰æ‹©æµ‹è¯•é›†ä¸­å­˜åœ¨çš„ç‰¹å¾
                valid_cols = [c for c in selected_features if c in X_test_pre.columns]
                X_eval = X_test_pre[valid_cols].values
            else:
                X_eval = X_test_pre.values

            best_model_name = max(
                models_dict.keys(), 
                key=lambda n: roc_auc_score(y_test, models_dict[n].predict_proba(X_eval)[:, 1])
            )
            _log(f"é€‰å®šæœ€ä½³æ¨¡å‹è¿›è¡Œè¯Šæ–­å¯è§†åŒ–: {best_model_name}", "OK")

        except Exception as e:
            _log(f"èµ„äº§è§£æå¤±è´¥ ({target}): {e}", "ERR")
            continue

        target_thresholds, target_perf_report = {}, []

        # 2. éå†æ¨¡å‹ï¼šè®¡ç®—é˜ˆå€¼ä¸å¤šç»´æ•ˆèƒ½
        for name, clf in models_dict.items():
            y_prob = clf.predict_proba(X_eval)[:, 1]
            fpr, tpr, thresholds = roc_curve(y_test, y_prob)
            auc_val = roc_auc_score(y_test, y_prob)
            
            # Youden Index å¯»ä¼˜ (å…¨äººç¾¤)
            if len(thresholds) <= 1:
                best_th = 0.5
            else:
                youden_index = tpr + (1 - fpr) - 1
                best_th = float(thresholds[np.argmax(youden_index)])
            
            best_th = min(1.0, best_th) # ä¿®æ­£éæ³•é˜ˆå€¼
            target_thresholds[name] = best_th

            # --- å…¨äººç¾¤æ•ˆèƒ½å®¡è®¡ ---
            perf_main = calculate_detailed_metrics(y_test, y_prob, best_th)
            perf_main.update({
                'Algorithm': name, 
                'Group': 'Full Population', 
                'Outcome': target,
                'AUC': round(auc_val, 4) # ã€æ–°å¢ã€‘åŠ å…¥ AUC
            })
            target_perf_report.append(perf_main)

            # --- äºšç»„æ•ˆèƒ½å®¡è®¡ (è‚¾ç—…äºšç»„ä¿æŠ¤é€»è¾‘) ---
            if 'sub_mask' in eval_data:
                mask = eval_data['sub_mask']
                # ã€æ–°å¢ï¼šæ ·æœ¬é‡ä¿æŠ¤æ£€æŸ¥ã€‘
                if mask.sum() > 10 and len(np.unique(y_test[mask])) > 1:
                    y_prob_sub = y_prob[mask]
                    y_test_sub = y_test[mask]
                    
                    # ã€ä¼˜åŒ–ï¼šè®¡ç®—äºšç»„ç‹¬ç«‹æœ€ä¼˜æˆªæ–­å€¼ã€‘
                    fpr_s, tpr_s, th_s = roc_curve(y_test_sub, y_prob_sub)
                    youden_s = tpr_s + (1 - fpr_s) - 1
                    best_th_sub = float(th_s[np.argmax(youden_s)])
                    
                    # ä½¿ç”¨ä¸»äººç¾¤é˜ˆå€¼è¯„ä¼°å½“å‰æ€§èƒ½
                    perf_sub = calculate_detailed_metrics(y_test_sub, y_prob_sub, best_th)
                    perf_sub.update({
                        'Algorithm': name, 
                        'Group': 'Subgroup (Non-Renal)', 
                        'Outcome': target,
                        'AUC': round(roc_auc_score(y_test_sub, y_prob_sub), 4),
                        'Subgroup_Specific_Th': round(best_th_sub, 4) # å­˜å‚¨ç‹¬ç«‹çš„å»ºè®®é˜ˆå€¼
                    })
                    target_perf_report.append(perf_sub)

            # ä»…ä¸ºæœ€ä½³æ¨¡å‹ç”Ÿæˆè¯Šæ–­å¯è§†åŒ–å›¾ï¼Œé¿å…å›¾ç‰‡å†—ä½™
            if name == best_model_name:
                plot_diagnostic_viz(y_test, y_prob, best_th, name, target, fig_save_dir)

        # 3. èµ„äº§æŒä¹…åŒ–
        # ä¿å­˜é˜ˆå€¼ JSON (ç”¨äº eICU å¤–éƒ¨éªŒè¯ä¸€é”®æ˜ å°„)
        with open(os.path.join(target_dir, "thresholds.json"), 'w') as f:
            json.dump(target_thresholds, f, indent=4)
        
        # å­˜å…¥ç»“å±€å­ç›®å½•ä¸ Table æ±‡æ€»ç›®å½•
        perf_df = pd.DataFrame(target_perf_report)
        perf_df.to_csv(os.path.join(target_dir, "internal_diagnostic_perf.csv"), index=False)
        perf_df.to_csv(os.path.join(SUPP_TABLE_DIR, f"Table3_Perf_{target}.csv"), index=False)
        
        global_summary.extend(target_perf_report)
        
        # å®æ—¶åé¦ˆ
        best_perf = next(p for p in target_perf_report if p['Algorithm'] == best_model_name and p['Group'] == 'Full Population')
        _log(f"å®¡è®¡å®Œæˆã€‚æœ€ä¼˜æ¨¡å‹ F1: {best_perf['F1_Score']} (AUC: {best_perf['AUC']})", "OK")

    # 4. å…¨å±€æ±‡æ€»å¹¶æŒ‰å­¦æœ¯é€»è¾‘æ’åº
    if global_summary:
        summary_df = pd.DataFrame(global_summary)
        # æ’åºï¼šç»“å±€å‡åº -> åˆ†ç»„å‡åº -> AUC é™åº
        summary_df = summary_df.sort_values(['Outcome', 'Group', 'AUC'], ascending=[True, True, False])
        summary_df.to_csv(os.path.join(MODEL_ROOT, "global_diagnostic_summary.csv"), index=False)
        _log(f"ä»»åŠ¡åœ†æ»¡å®Œæˆï¼å…¨å±€æŠ¥å‘Šè§: {MODEL_ROOT}/global_diagnostic_summary.csv", "OK")

if __name__ == "__main__":
    # 1. æ‰§è¡Œ 07 æ­¥ä¸»æµç¨‹ï¼šè®¡ç®—é˜ˆå€¼ä¸æ•ˆèƒ½
    run_cutoff_optimization_flow()
    
    # 2. è‡ªåŠ¨å¯¼å‡º Table 3 (Excel æˆ– CSV)
    export_formatted_table3()
    
    # 3. å®ä¾‹åŒ–å¯è§†åŒ–å·¥å…·
    viz = ResultVisualizer(base_dir=BASE_DIR)
    
    # 4. ç”Ÿæˆç‰¹å¾é‡è¦æ€§æ±‡æ€»å›¾ (åŸºäºç¬¬ 7 æ­¥åçš„èµ„äº§)
    viz.summarize_feature_importance(outcomes=OUTCOMES, top_n=15)
    
    # 5. ã€å…³é”®ä¿®å¤ã€‘ç”Ÿæˆæ£®æ—å›¾ï¼šå¼ºåˆ¶ä½¿ç”¨ç¬¬ 06 æ­¥ç”Ÿæˆçš„ç½®ä¿¡åŒºé—´æŠ¥å‘Š
    step7_report_path = os.path.join(MODEL_ROOT, "performance_report.csv")
    
    if os.path.exists(step7_report_path):
        _log(f"æ­£åœ¨åŸºäºç¬¬ 06 æ­¥çš„ç½®ä¿¡åŒºé—´æ•°æ®ç”Ÿæˆæ£®æ—å›¾: {step7_report_path}", "INFO")
        viz.report_path = step7_report_path
        viz.plot_performance_forest()
    else:
        _log("è­¦å‘Š: æœªæ‰¾åˆ°ç¬¬ 06 æ­¥çš„ performance_report.csvã€‚", "WARN")
        _log("è¯·ç¡®ä¿å·²è¿è¡Œç¬¬ 06 æ­¥è„šæœ¬ï¼Œæ£®æ—å›¾éœ€è¦å…¶æä¾›çš„ 95% CI æ•°æ®ã€‚", "WARN")

    _log(f"07 æ­¥å›¾ç‰‡è¾“å‡ºç›®å½•: {os.path.abspath(FIG_CUTOFF_DIR)}", "OK")
